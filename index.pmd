% Homework 3 MPI and OpenMP Programming
% Jon Craton

Programming Assignment 1
========================

> (20 points) We have learned parallel sieve of Eratosthenes algorithm. To find all prime numbers up to some limit value *N*, this algorithm works as follows:
> 
> 1.  Create the sieve -- an array indexed from 2 to N. (This can be an
>     array of bools or ints or anything else that supports random access
>     and provides entries that are easy to “mark”.)
>
> 2.  Set *k* to 2.
>
> 3.  Loop:
> 
>     1.  In the sieve, mark all entries whose indices are multiples of
>         *k* between *k^2^* and *N*, inclusive.
> 
>     2.  Find the smallest index greater than *k* that is unmarked, and
>         set *k* to this value.
> 
> Until *k^2^* &gt; *N*.
> 
> 1.  The indices of the unmarked sieve entries are prime numbers.
> 
> We have discussed how to convert the sequential program into parallel program in our class. We also identified a few ways to improve the algorithm. In this exercise, three versions of sieve program are given, i.e., sieve1.c, sieve2.c, sieve3.c. Go through the code and answer the following questions:

Part 1
------

> (5 points) Read the code and briefly explain how each version works.

All three version use MPI programming to perform the sieve of Eratosthenes algorithm.

I'll focus on describing the methods used in the first version as the next two are discussed in the following sections.

We start by initializing MPI:

```c
MPI_Init (&argc, &argv);
/* start timer */
MPI_Barrier(MPI_COMM_WORLD);
elapsed_time = -MPI_Wtime();
MPI_Comm_rank (MPI_COMM_WORLD, &id);
MPI_Comm_size (MPI_COMM_WORLD, &p);
```

We then check out command line args and get our size from the first argument.

Next, we assign elements to each process in our program:

```c
low_value = 2 + BLOCK_LOW(id,p,n-1);
high_value = 2 + BLOCK_HIGH(id,p,n-1);
size = BLOCK_SIZE(id,p,n-1);
proc0_size = (n-1)/p;
```

Then we politely ask the kernel for a slice of RAM to use for marking primes:

```c
marked = (int *) malloc (size * sizeof(int));
```

Next, we have the meat of the algorithm:

```c
prime = 2;
do {
 if (prime * prime > low_value)
  first = prime * prime - low_value;
 else {
  if (!(low_value % prime)) first = 0;
  else first = prime - (low_value % prime);
 }
 for (i = first; i < size; i += prime) marked[i] = 1;
 if (!id) {
  while (marked[++index]);
   prime = index + 2;
 }
 MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
} while (prime * prime <= n);
```

The first `if` block in the loop is concerned with determining the best starting value for the upcoming loop that will mark our primes. We don't need to mark values less than $prime^2$, as they will have already been marked. This gets a little more complicated than the sequential version, as we have to consider our block offset, but the concept is the same.

```c
if (prime * prime > low_value)
 first = prime * prime - low_value;
else {
 if (!(low_value % prime)) first = 0;
 else first = prime - (low_value % prime);
}
```

Next, we simply mark our values:

```c
for (i = first; i < size; i += prime) marked[i] = 1;
```

Now, we find the next prime to use for marking:

```c
if (!id) {
  while (marked[++index]);
   prime = index + 2;
}
```

We only do this in our root process, as we will be sending this information to other processes:

```c
MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
```

We then proceed to perform the marking of this block again and repeat untilwe hit $prime^2$.

```c
} while (prime * prime <= n);
```

After the marking is complete, each process counts its primes:

```c
count = 0;

for (i = 0; i < size; i++)
 if (!marked[i]) count++;
```

MPI is then used to reduce these values from all process into a single global sum in the root process:

```c
MPI_Reduce (&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
```

Finally, we measure execution time, print our results, and close things down.

Part 2
------

> (5 points) What are the differences between sieve1.c and seive2.c? How does sieve2.c improve on sieve1.c?

The differences are:

```diff
diff --git a/sieve1.c b/sieve2.c
index 8d3d8c0..d41b7a7 100755
--- a/sieve1.c
+++ b/sieve2.c
@@ -72,11 +72,19 @@ int main (int argc, char *argv[])
   exit (1);
  }
 
- for (i = 0; i < size; i++) marked[i] = 0;
-
- if (!id) index = 0;
+ for (i = 0; i < size; i++)
+  if (!(low_value % 2))
+   marked[i] = i%2==0 ? 1:0;
+  else
+   marked[i] = i%2==0 ? 0:1;
 
- prime = 2;
+ if (!id)
+ {
+  index = 1;
+  marked[0] = 0;
+ }
+ 
+ prime=3;
  do {
   if (prime * prime > low_value)
    first = prime * prime - low_value;
@@ -85,9 +93,11 @@ int main (int argc, char *argv[])
    else first = prime - (low_value % prime);
   }
   for (i = first; i < size; i += prime) marked[i] = 1;
+
   if (!id) {
-   while (marked[++index]);
-    prime = index + 2;
+   while (marked[index]) index += 2;
+   prime = index + 2;
+   index += 2;
   }
   MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
  } while (prime * prime <= n);
```

We can see from this diff that version 2 takes advantage of the knowledge that no even numbers are prime. When initializing the sieve, version 2 initializes all even numbers to start marked:

```diff
- for (i = 0; i < size; i++) marked[i] = 0;
-
- if (!id) index = 0;
+ for (i = 0; i < size; i++)
+  if (!(low_value % 2))
+   marked[i] = i%2==0 ? 1:0;
+  else
+   marked[i] = i%2==0 ? 0:1;
```

This also allows the search for primes to complete more quickly as only half as many values are searched:

```diff
-   while (marked[++index]);
-    prime = index + 2;
+   while (marked[index]) index += 2;
+   prime = index + 2;
+   index += 2;
```

Part 3
------

> (5 points) What are the differences between sieve1.c and seive3.c? How does sieve3.c improve on sieve1.c?

The differences are:

```diff
diff --git a/sieve1.c b/sieve3.c
index 8d3d8c0..7f5127e 100755
--- a/sieve1.c
+++ b/sieve3.c
@@ -24,7 +24,9 @@ int main (int argc, char *argv[])
  int index;		/* index of the current sieve	*/
  int low_value;	/* lowest value assigned to this process */
  int *marked;		/* array elements to be  marked	*/
+ int *sieves;		/* array of sieving primes from k -> sqrt(n)	*/
  int n;		/* value of the largest number	*/
+ int sqrtN;		/* square root of n	*/
  int p; 		/* number of processes		*/
  int proc0_size;	/* number of elements assigned to process zero */
 			/* this is to find if process zero has all primes */
@@ -54,7 +56,8 @@ int main (int argc, char *argv[])
  high_value = 2 + BLOCK_HIGH(id,p,n-1);
  size = BLOCK_SIZE(id,p,n-1);
  proc0_size = (n-1)/p;
- if ((2 + proc0_size) < (int) sqrt((double) n))
+ sqrtN = (int) sqrt((double) n);
+ if ((2 + proc0_size) < sqrtN)
  {
   if (!id)
    printf ("Too many processes\n");
@@ -64,19 +67,41 @@ int main (int argc, char *argv[])
  }
 
  marked = (int *) malloc (size * sizeof(int));
+ sieves = (int *) malloc ((sqrtN) * sizeof(int));
 
- if (marked == NULL)
+ if ((marked == NULL) || (sieves == NULL))
  {
   printf ("Cannot allocate enough memory\n");
   MPI_Finalize();
   exit (1);
  }
 
- for (i = 0; i < size; i++) marked[i] = 0;
+ for (i = 0; i < size; i++)
+  if (!(low_value % 2))
+   marked[i] = i%2==0 ? 1:0;
+  else
+   marked[i] = i%2==0 ? 0:1;
+
+ for (i=0; i < (sqrtN-2); i++) sieves[i]= i%2==0 ? 1:0;
 
- if (!id) index = 0;
+ if (!id)
+  marked[0] = 0;
 
- prime = 2;
+ //Create a list of sieving primes on each process from 2 -> SQRT(n)
+ sieves[0] = 0;
+ index = 1;
+ prime = 3;
+ do 
+ {
+  for(i=prime-2+prime; i<(sqrtN-2); i+=prime) sieves[i] = 1;
+  while (sieves[index]) index += 2;
+  prime = index + 2;
+  index += 2;
+ } while (prime * prime <= sqrtN);
+
+ //Mark all of the sieving primes from 2 -> n
+ index = 1;
+ prime = 3;
  do {
   if (prime * prime > low_value)
    first = prime * prime - low_value;
@@ -85,18 +110,19 @@ int main (int argc, char *argv[])
    else first = prime - (low_value % prime);
   }
   for (i = first; i < size; i += prime) marked[i] = 1;
-  if (!id) {
-   while (marked[++index]);
-    prime = index + 2;
-  }
-  MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
+
+  while (sieves[index]) index += 2;
+  prime = index + 2;
+  index += 2;
+
  } while (prime * prime <= n);
 
+ //Count the primes in each process 
  count = 0;
-
  for (i = 0; i < size; i++)
   if (!marked[i]) count++;
 
+ //SUM each count into process 0.
  MPI_Reduce (&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
 
  elapsed_time += MPI_Wtime();
```

-   (5 points) Benchmark the performance of the three versions on the
    Rushmore cluster using different number of processors and fill the
    table below. Does the Benchmark results meet your expectation? Why?

I've completed trial runs using primes up to 100,000,000

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9

I can't say I was expecting these results, but I think I can understand why they happen to a certain extent. Similar performance when Np=1 makes sense. From there, we do worse as we are paying a communication cost for multiprocessing. We begin to see benefit at Np=3 and real benefit at Np=4.

Bonus
-----

> (**Bonus 10 points**) If you are able to find another way to optimize the sieve program and perform better than the three visions provided in the exercise, you have will chance to get an extra 10 points for this homework.

Let's first explore the hardware that we are working with. `lshw` indicates the following:

- Xeon Silver 4110 @ 2.1GHz
- 16KiB L1 Cache
- 256KiB L2 Cache

Let's try to stay in L2. We'll target 128KiB blocks to be sure we stay inside it. Here's what the inner loops looks like after we adjust it to have each process handle primes in blocks:

```c
// Split into blocks to make use of cache locality
int blocksize = 1024 * 16 / sizeof(int);
int block_start = 0;
for (low_value; low_value <= high_value; low_value+=blocksize) {
 index = 1;
 prime = 3;
 int inner_size = high_value - low_value;
 if (inner_size > blocksize) inner_size = blocksize;

 do {
  if (prime * prime > low_value)
   first = prime * prime - low_value;
  else {
   if (!(low_value % prime)) first = 0;
   else first = prime - (low_value % prime);
  }
  for (i = first; i <= inner_size; i += prime) marked[i+block_start] = 1;

  while (sieves[index]) index += 2;
  prime = index + 2;
  index += 2;

 } while (prime * prime <= n);

 block_start += blocksize;
}
```

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8

That gives us better than a 3x boost. Np=1 outperforms previous algorithms with Np=4.

We can do a little better on memory performance still. We are storing all the even numbers in RAM even though we know they are not prime. We can adjust our program to not store them at all. This will save RAM and allow us to increase our block size.

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8
  sieve5.c   1.2    1.1    0.9    0.6

That didn't help by as much, but still gives us another 25%.

As a final quick optimization, we can change from ints to chars and bump up our cache size by 4x. I also tested more complex bit-based storage, but this turned out to not improve performance significantly.

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8
  sieve5.c   1.2    1.1    0.9    0.6
  sieve6.c   1.0    1.1    0.7    0.5

That gives us another modest 10-20% boost.

(10 points) Assignment 2: Compare the differences of MPI and openMP
programming. Which one do you like better and why?

(30 points) Assignment 3: For linpack\_bench.cpp, read the profile tool
gprof user manual (see the html guide), run the profiling tool and
report the percentage of running times for each function.

(40 points) Programming Assignment 4: For the 4 sequential c programs,
p1.c, p2.c, p3.c, p4.c, using openmp to parallelize them as much as
possible and do the following profiling.

-   Add timestamp functions to the code to count the running time of the
    sequential programs and your openmp programs and show the result in
    the following table.

-   Copy your openmp program running outputs to a report file
    running.txt. You need to make sure openmp programs should generate
    the same results as the sequential codes.

                                 P1   P2   P3   P4
  ------------------------------ ---- ---- ---- ----
  Sequential code running time                  
  openMP running time                           
  Speed up                                      
  No. of threads                                


Due: Nov 19, by Midnight 2019

Total: 100 points

Note: For all the programming assignments, you can choose any operating
systems you want. I will usually provide C/C++ samples for the
programming assignments. If you prefer to use other languages, e.g.,
Java, they are accepted too. A README.txt is required to submit any
programming assignments. In the README.txt, you need to provide the
following information:

1)  How to compile your program?

2)  How to run your program?

3)  What is the output and the results when I run your program?

4)  Any descriptions which may help me to compile, run, and verify your
    answers. (FYI: I check each programming assignment turned in!)

Zip all you source code, project files, supporting files, and README.txt
and submit the all-in-one zip file together to the D2L Dropbox.

