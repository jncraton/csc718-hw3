% Homework 3 MPI and OpenMP Programming
% Jon Craton

Programming Assignment 1
========================

> (20 points) We have learned parallel sieve of Eratosthenes algorithm. To find all prime numbers up to some limit value *N*, this algorithm works as follows:
> 
> 1.  Create the sieve -- an array indexed from 2 to N. (This can be an
>     array of bools or ints or anything else that supports random access
>     and provides entries that are easy to “mark”.)
>
> 2.  Set *k* to 2.
>
> 3.  Loop:
> 
>     1.  In the sieve, mark all entries whose indices are multiples of
>         *k* between *k^2^* and *N*, inclusive.
> 
>     2.  Find the smallest index greater than *k* that is unmarked, and
>         set *k* to this value.
> 
> Until *k^2^* &gt; *N*.
> 
> 1.  The indices of the unmarked sieve entries are prime numbers.
> 
> We have discussed how to convert the sequential program into parallel program in our class. We also identified a few ways to improve the algorithm. In this exercise, three versions of sieve program are given, i.e., sieve1.c, sieve2.c, sieve3.c. Go through the code and answer the following questions:

Part 1
------

> (5 points) Read the code and briefly explain how each version works.

All three version use MPI programming to perform the sieve of Eratosthenes algorithm.

I'll focus on describing the methods used in the first version as the next two are discussed in the following sections.

We start by initializing MPI:

```c
MPI_Init (&argc, &argv);
/* start timer */
MPI_Barrier(MPI_COMM_WORLD);
elapsed_time = -MPI_Wtime();
MPI_Comm_rank (MPI_COMM_WORLD, &id);
MPI_Comm_size (MPI_COMM_WORLD, &p);
```

We then check out command line args and get our size from the first argument.

Next, we assign elements to each process in our program:

```c
low_value = 2 + BLOCK_LOW(id,p,n-1);
high_value = 2 + BLOCK_HIGH(id,p,n-1);
size = BLOCK_SIZE(id,p,n-1);
proc0_size = (n-1)/p;
```

Then we politely ask the kernel for a slice of RAM to use for marking primes:

```c
marked = (int *) malloc (size * sizeof(int));
```

Next, we have the meat of the algorithm:

```c
prime = 2;
do {
 if (prime * prime > low_value)
  first = prime * prime - low_value;
 else {
  if (!(low_value % prime)) first = 0;
  else first = prime - (low_value % prime);
 }
 for (i = first; i < size; i += prime) marked[i] = 1;
 if (!id) {
  while (marked[++index]);
   prime = index + 2;
 }
 MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
} while (prime * prime <= n);
```

The first `if` block in the loop is concerned with determining the best starting value for the upcoming loop that will mark our primes. We don't need to mark values less than $prime^2$, as they will have already been marked. This gets a little more complicated than the sequential version, as we have to consider our block offset, but the concept is the same.

```c
if (prime * prime > low_value)
 first = prime * prime - low_value;
else {
 if (!(low_value % prime)) first = 0;
 else first = prime - (low_value % prime);
}
```

Next, we simply mark our values:

```c
for (i = first; i < size; i += prime) marked[i] = 1;
```

Now, we find the next prime to use for marking:

```c
if (!id) {
  while (marked[++index]);
   prime = index + 2;
}
```

We only do this in our root process, as we will be sending this information to other processes:

```c
MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
```

We then proceed to perform the marking of this block again and repeat untilwe hit $prime^2$.

```c
} while (prime * prime <= n);
```

After the marking is complete, each process counts its primes:

```c
count = 0;

for (i = 0; i < size; i++)
 if (!marked[i]) count++;
```

MPI is then used to reduce these values from all process into a single global sum in the root process:

```c
MPI_Reduce (&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
```

Finally, we measure execution time, print our results, and close things down.

Part 2
------

> (5 points) What are the differences between sieve1.c and seive2.c? How does sieve2.c improve on sieve1.c?

The differences are:

```diff
diff --git a/sieve1.c b/sieve2.c
index 8d3d8c0..d41b7a7 100755
--- a/sieve1.c
+++ b/sieve2.c
@@ -72,11 +72,19 @@ int main (int argc, char *argv[])
   exit (1);
  }
 
- for (i = 0; i < size; i++) marked[i] = 0;
-
- if (!id) index = 0;
+ for (i = 0; i < size; i++)
+  if (!(low_value % 2))
+   marked[i] = i%2==0 ? 1:0;
+  else
+   marked[i] = i%2==0 ? 0:1;
 
- prime = 2;
+ if (!id)
+ {
+  index = 1;
+  marked[0] = 0;
+ }
+ 
+ prime=3;
  do {
   if (prime * prime > low_value)
    first = prime * prime - low_value;
@@ -85,9 +93,11 @@ int main (int argc, char *argv[])
    else first = prime - (low_value % prime);
   }
   for (i = first; i < size; i += prime) marked[i] = 1;
+
   if (!id) {
-   while (marked[++index]);
-    prime = index + 2;
+   while (marked[index]) index += 2;
+   prime = index + 2;
+   index += 2;
   }
   MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
  } while (prime * prime <= n);
```

We can see from this diff that version 2 takes advantage of the knowledge that no even numbers are prime. When initializing the sieve, version 2 initializes all even numbers to start marked:

```diff
- for (i = 0; i < size; i++) marked[i] = 0;
-
- if (!id) index = 0;
+ for (i = 0; i < size; i++)
+  if (!(low_value % 2))
+   marked[i] = i%2==0 ? 1:0;
+  else
+   marked[i] = i%2==0 ? 0:1;
```

This also allows the search for primes to complete more quickly as only half as many values are searched:

```diff
-   while (marked[++index]);
-    prime = index + 2;
+   while (marked[index]) index += 2;
+   prime = index + 2;
+   index += 2;
```

Part 3
------

> (5 points) What are the differences between sieve1.c and seive3.c? How does sieve3.c improve on sieve1.c?

The differences are:

```diff
diff --git a/sieve1.c b/sieve3.c
index 8d3d8c0..7f5127e 100755
--- a/sieve1.c
+++ b/sieve3.c
@@ -24,7 +24,9 @@ int main (int argc, char *argv[])
  int index;		/* index of the current sieve	*/
  int low_value;	/* lowest value assigned to this process */
  int *marked;		/* array elements to be  marked	*/
+ int *sieves;		/* array of sieving primes from k -> sqrt(n)	*/
  int n;		/* value of the largest number	*/
+ int sqrtN;		/* square root of n	*/
  int p; 		/* number of processes		*/
  int proc0_size;	/* number of elements assigned to process zero */
 			/* this is to find if process zero has all primes */
@@ -54,7 +56,8 @@ int main (int argc, char *argv[])
  high_value = 2 + BLOCK_HIGH(id,p,n-1);
  size = BLOCK_SIZE(id,p,n-1);
  proc0_size = (n-1)/p;
- if ((2 + proc0_size) < (int) sqrt((double) n))
+ sqrtN = (int) sqrt((double) n);
+ if ((2 + proc0_size) < sqrtN)
  {
   if (!id)
    printf ("Too many processes\n");
@@ -64,19 +67,41 @@ int main (int argc, char *argv[])
  }
 
  marked = (int *) malloc (size * sizeof(int));
+ sieves = (int *) malloc ((sqrtN) * sizeof(int));
 
- if (marked == NULL)
+ if ((marked == NULL) || (sieves == NULL))
  {
   printf ("Cannot allocate enough memory\n");
   MPI_Finalize();
   exit (1);
  }
 
- for (i = 0; i < size; i++) marked[i] = 0;
+ for (i = 0; i < size; i++)
+  if (!(low_value % 2))
+   marked[i] = i%2==0 ? 1:0;
+  else
+   marked[i] = i%2==0 ? 0:1;
+
+ for (i=0; i < (sqrtN-2); i++) sieves[i]= i%2==0 ? 1:0;
 
- if (!id) index = 0;
+ if (!id)
+  marked[0] = 0;
 
- prime = 2;
+ //Create a list of sieving primes on each process from 2 -> SQRT(n)
+ sieves[0] = 0;
+ index = 1;
+ prime = 3;
+ do 
+ {
+  for(i=prime-2+prime; i<(sqrtN-2); i+=prime) sieves[i] = 1;
+  while (sieves[index]) index += 2;
+  prime = index + 2;
+  index += 2;
+ } while (prime * prime <= sqrtN);
+
+ //Mark all of the sieving primes from 2 -> n
+ index = 1;
+ prime = 3;
  do {
   if (prime * prime > low_value)
    first = prime * prime - low_value;
@@ -85,18 +110,19 @@ int main (int argc, char *argv[])
    else first = prime - (low_value % prime);
   }
   for (i = first; i < size; i += prime) marked[i] = 1;
-  if (!id) {
-   while (marked[++index]);
-    prime = index + 2;
-  }
-  MPI_Bcast (&prime,  1, MPI_INT, 0, MPI_COMM_WORLD);
+
+  while (sieves[index]) index += 2;
+  prime = index + 2;
+  index += 2;
+
  } while (prime * prime <= n);
 
+ //Count the primes in each process 
  count = 0;
-
  for (i = 0; i < size; i++)
   if (!marked[i]) count++;
 
+ //SUM each count into process 0.
  MPI_Reduce (&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
 
  elapsed_time += MPI_Wtime();
```

-   (5 points) Benchmark the performance of the three versions on the
    Rushmore cluster using different number of processors and fill the
    table below. Does the Benchmark results meet your expectation? Why?

I've completed trial runs using primes up to 100,000,000

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9

I can't say I was expecting these results, but I think I can understand why they happen to a certain extent. Similar performance when Np=1 makes sense. From there, we do worse as we are paying a communication cost for multiprocessing. We begin to see benefit at Np=3 and real benefit at Np=4.

Bonus
-----

> (**Bonus 10 points**) If you are able to find another way to optimize the sieve program and perform better than the three visions provided in the exercise, you have will chance to get an extra 10 points for this homework.

Let's first explore the hardware that we are working with. `lshw` indicates the following:

- Xeon Silver 4110 @ 2.1GHz
- 16KiB L1 Cache
- 256KiB L2 Cache

Let's try to stay in L2. We'll target 128KiB blocks to be sure we stay inside it. Here's what the inner loops looks like after we adjust it to have each process handle primes in blocks:

```c
// Split into blocks to make use of cache locality
int blocksize = 1024 * 16 / sizeof(int);
int block_start = 0;
for (low_value; low_value <= high_value; low_value+=blocksize) {
 index = 1;
 prime = 3;
 int inner_size = high_value - low_value;
 if (inner_size > blocksize) inner_size = blocksize;

 do {
  if (prime * prime > low_value)
   first = prime * prime - low_value;
  else {
   if (!(low_value % prime)) first = 0;
   else first = prime - (low_value % prime);
  }
  for (i = first; i <= inner_size; i += prime) marked[i+block_start] = 1;

  while (sieves[index]) index += 2;
  prime = index + 2;
  index += 2;

 } while (prime * prime <= n);

 block_start += blocksize;
}
```

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8

That gives us better than a 3x boost. Np=1 outperforms previous algorithms with Np=4.

We can do a little better on memory performance still. We are storing all the even numbers in RAM even though we know they are not prime. We can adjust our program to not store evens at all. This will save RAM and allow us to increase our block size.

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8
  sieve5.c   1.2    1.1    0.9    0.6

That didn't help by as much, but still gives us another 25%.

We can also change from ints to chars and bump up our cache size by 4x. I also tested more complex bit-based storage, but this turned out to not improve performance significantly.

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8
  sieve5.c   1.2    1.1    0.9    0.6
  sieve6.c   1.0    1.1    0.7    0.5

That gives us another modest 10-20% boost.

It occurred to me that we actually don't need all the RAM we're currently allocating. We should be able to do everything in L2 cache, no longer be memory bound, and benefit even more from cache locality.

Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8
  sieve5.c   1.2    1.1    0.9    0.6
  sieve6.c   1.0    1.1    0.7    0.5
  sieve7.c   0.7    0.5    0.34   0.26

That gives us a nearly 2x increase.

Next let's try unrolling the main loop. Here are the performance numbers:

  Problem    Np=1   Np=2   Np=3   Np=4
  ---------- ------ ------ ------ ------
  sieve1.c   4.0    5.9    3.9    3.1         
  sieve2.c   4.0    5.7    3.9    3.1
  sieve3.c   4.0    5.7    3.8    2.9
  sieve4.c   1.8    1.9    1.3    0.8
  sieve5.c   1.2    1.1    0.9    0.6
  sieve6.c   1.0    1.1    0.7    0.5
  sieve7.c   0.7    0.5    0.34   0.26
  sieve8.c   0.55   0.32   0.22   0.19

That gives us another nice bump of perhaps 25%.

I did make one last version of this program using 64-bit ints so that I could count to higher numbers. I was able to count the primes between 1 and 100 billion in 255 seconds. Somewhat frustratingly, my result disagrees with the real result by exactly 1.

Part 2
------

> (10 points) Assignment 2: Compare the differences of MPI and openMP programming. Which one do you like better and why?

These two ways of programming differ in a number of ways. Perhaps the most significant is the way that memory is treated. MPI programming acknowledges that discrete hosts in a modern supercomputing system have their own memory and CPU that will perform best locally. MPI provides an API to easily orchestrate these machines into a group and efficiently share data between them as needed. OpenMP is designed around the concept of shared memory, as we see in more traditional monolithic computer systems.

It's hard for me to say which I like better. I've spend more time with MPI, and I like the power that it gives me to run code across a number of hosts. However, most of the time I'm not writing code for a supercomputer. Instead, I'm writing code for my mulitcore workstation, or perhaps a single server multicore server. In that case, I'd prefer to use OpenMP in order to get the cleaner shared memory interface.

Part 3
------

> (30 points) Assignment 3: For linpack\_bench.cpp, read the profile tool gprof user manual (see the html guide), run the profiling tool and report the percentage of running times for each function.

A makefile is included in the `src/Q3` directory showing the steps need to generate this output. The output itself is:

```
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 83.23      0.99     0.99   501499     0.00     0.00  daxpy(int, double, double*, int, double*, int)
  6.73      1.07     0.08  2000000     0.00     0.00  r8_random(int*)
  5.04      1.13     0.06        1     0.06     1.05  dgefa(double*, int, int, int*)
  4.20      1.18     0.05                             main
  0.84      1.19     0.01        2     0.01     0.05  r8_matgen(int, int)
  0.00      1.19     0.00  1003000     0.00     0.00  r8_max(double, double)
  0.00      1.19     0.00   509110     0.00     0.00  r8_abs(double)
  0.00      1.19     0.00      999     0.00     0.00  dscal(int, double, double*, int)
  0.00      1.19     0.00      999     0.00     0.00  idamax(int, double*, int)
  0.00      1.19     0.00       11     0.00     0.00  std::setw(int)
  0.00      1.19     0.00        4     0.00     0.00  cpu_time()
  0.00      1.19     0.00        2     0.00     0.00  timestamp()
  0.00      1.19     0.00        1     0.00     0.00  _GLOBAL__sub_I_main
  0.00      1.19     0.00        1     0.00     0.00  r8_epsilon()
  0.00      1.19     0.00        1     0.00     0.00  __static_initialization_and_destruction_0(int, int)
  0.00      1.19     0.00        1     0.00     0.00  dgesl(double*, int, int, int*, double*, int)
```

We can see that the program spends the majority of its running time (83.23%) in the function daxpy.

Part 4
------

> (40 points) Programming Assignment 4: For the 4 sequential c programs, p1.c, p2.c, p3.c, p4.c, using openmp to parallelize them as much as possible and do the following profiling.
>
> -   Add timestamp functions to the code to count the running time of the sequential programs and your openmp programs and show the result in the following table.
>
> -   Copy your openmp program running outputs to a report file running.txt. You need to make sure openmp programs should generate the same results as the sequential codes.

Running times were collected by averaging using a command such as:

```bash
time for i in {1..100}; do ./p1 > /dev/null; done
```

Reported times are the real time averages from those commands. I've used 4 threads for the multithreaded testing. It should be noted that the laptop I used for this is only dual core with 2 threads per core, so it would be difficult to come close to 4x performance.

                                 P1   P2   P3   P4
  ------------------------------ ---- ---- ---- ----
  Sequential code running time   .017 .013 1.0  .054             
  openMP running time            .007 .015              
  Speed up                       2.4  -.2             
  No. of threads                 4    4           


Due: Nov 19, by Midnight 2019

Total: 100 points

Note: For all the programming assignments, you can choose any operating
systems you want. I will usually provide C/C++ samples for the
programming assignments. If you prefer to use other languages, e.g.,
Java, they are accepted too. A README.txt is required to submit any
programming assignments. In the README.txt, you need to provide the
following information:

1)  How to compile your program?

2)  How to run your program?

3)  What is the output and the results when I run your program?

4)  Any descriptions which may help me to compile, run, and verify your
    answers. (FYI: I check each programming assignment turned in!)

Zip all you source code, project files, supporting files, and README.txt
and submit the all-in-one zip file together to the D2L Dropbox.

